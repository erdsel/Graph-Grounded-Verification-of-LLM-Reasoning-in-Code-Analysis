# =============================================================================
# LLM CLIENT MODÃœLÃœ
# =============================================================================
# Bu modÃ¼l, BÃ¼yÃ¼k Dil Modelleri (LLM) ile iletiÅŸim kurar ve kod analizi
# iÃ§in reasoning (muhakeme) Ã§Ä±ktÄ±larÄ± Ã¼retir.
#
# Desteklenen LLM'ler:
# - OpenAI GPT modelleri (GPT-4, GPT-3.5-turbo)
# - Yerel/Alternatif API'ler (opsiyonel)
#
# Chain-of-Thought (CoT) Prompting:
# ---------------------------------
# CoT, LLM'lerin adÄ±m adÄ±m dÃ¼ÅŸÃ¼nmesini saÄŸlayan bir prompting tekniÄŸidir.
# Model, doÄŸrudan cevap vermek yerine dÃ¼ÅŸÃ¼nce sÃ¼recini aÃ§Ä±klar:
#
# Normal prompt: "Bu fonksiyon ne yapar?"
# CoT prompt: "Bu fonksiyonu adÄ±m adÄ±m analiz et. Her adÄ±mda ne olduÄŸunu aÃ§Ä±kla."
#
# Bu sayede modelin muhakeme sÃ¼recini gÃ¶rebilir ve doÄŸrulayabiliriz.
# =============================================================================

import os
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod

# OpenAI kÃ¼tÃ¼phanesi opsiyonel - kurulu deÄŸilse mock kullanÄ±lÄ±r
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Google Gemini kÃ¼tÃ¼phanesi
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Groq kÃ¼tÃ¼phanesi (OpenAI uyumlu)
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False


@dataclass
class LLMResponse:
    """
    LLM'den gelen yanÄ±tÄ± temsil eden veri sÄ±nÄ±fÄ±.

    Attributes:
        content: Ana yanÄ±t metni
        reasoning_steps: AyrÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ muhakeme adÄ±mlarÄ± (varsa)
        model: KullanÄ±lan model adÄ±
        usage: Token kullanÄ±m bilgileri
        raw_response: Ham API yanÄ±tÄ± (debug iÃ§in)
    """
    content: str
    reasoning_steps: List[str]
    model: str
    usage: Dict[str, int]
    raw_response: Optional[Dict] = None


class BaseLLMClient(ABC):
    """
    LLM client'larÄ± iÃ§in soyut temel sÄ±nÄ±f.

    FarklÄ± LLM saÄŸlayÄ±cÄ±larÄ± iÃ§in ortak arayÃ¼z tanÄ±mlar.
    Yeni bir LLM eklemek iÃ§in bu sÄ±nÄ±ftan tÃ¼retilir.
    """

    @abstractmethod
    def generate_reasoning(self, code: str, prompt_type: str = "analysis") -> LLMResponse:
        """
        Verilen kod iÃ§in LLM'den reasoning Ã§Ä±ktÄ±sÄ± Ã¼retir.

        Args:
            code: Analiz edilecek Python kodu
            prompt_type: Prompt tipi ("analysis", "explanation", "review")

        Returns:
            LLMResponse nesnesi
        """
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """
        LLM servisinin kullanÄ±labilir olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
        """
        pass


class OpenAIClient(BaseLLMClient):
    """
    OpenAI API ile iletiÅŸim kuran client sÄ±nÄ±fÄ±.

    KullanÄ±m:
        client = OpenAIClient(api_key="sk-...")
        response = client.generate_reasoning(kod, "analysis")
    """

    # FarklÄ± analiz tÃ¼rleri iÃ§in prompt ÅŸablonlarÄ±
    PROMPT_TEMPLATES = {
        "analysis": """
Sen bir kod analiz uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki Python kodunu detaylÄ± olarak analiz et.

KURALLAR:
1. Her adÄ±mÄ± numaralandÄ±rarak aÃ§Ä±kla
2. Fonksiyonlar arasÄ± Ã§aÄŸrÄ± iliÅŸkilerini belirt
3. Veri akÄ±ÅŸÄ±nÄ± takip et
4. DeÄŸiÅŸkenlerin nasÄ±l kullanÄ±ldÄ±ÄŸÄ±nÄ± aÃ§Ä±kla

FORMAT:
Her analiz adÄ±mÄ±nÄ± ÅŸu formatta yaz:
ADIM X: [AÃ§Ä±klama]
- Detay 1
- Detay 2

KOD:
```python
{code}
```

Analizi baÅŸlat:
""",

        "explanation": """
AÅŸaÄŸÄ±daki Python kodunu bir yazÄ±lÄ±m geliÅŸtiriciye aÃ§Ä±klar gibi anlat.

KOD:
```python
{code}
```

AÃ§Ä±klamanÄ± ÅŸu baÅŸlÄ±klar altÄ±nda yap:
1. GENEL BAKIÅ: Kodun amacÄ± nedir?
2. YAPISAL ANALÄ°Z: Hangi fonksiyonlar/sÄ±nÄ±flar var?
3. Ã‡AÄRI Ä°LÄ°ÅKÄ°LERÄ°: Hangi fonksiyon hangisini Ã§aÄŸÄ±rÄ±yor?
4. VERÄ° AKIÅI: Veriler nasÄ±l iÅŸleniyor?
""",

        "review": """
AÅŸaÄŸÄ±daki Python kodunu kod review perspektifinden deÄŸerlendir.

KOD:
```python
{code}
```

Her fonksiyon iÃ§in ÅŸunlarÄ± belirt:
- Ne iÅŸ yapÄ±yor
- Hangi fonksiyonlarÄ± Ã§aÄŸÄ±rÄ±yor
- Hangi deÄŸiÅŸkenleri kullanÄ±yor
- Potansiyel sorunlar (varsa)
""",

        "function_calls": """
AÅŸaÄŸÄ±daki Python kodundaki fonksiyon Ã§aÄŸrÄ± iliÅŸkilerini analiz et.

KOD:
```python
{code}
```

Her fonksiyon iÃ§in ÅŸu formatta yanÄ±t ver:
FONKSIYON: [fonksiyon_adÄ±]
Ã‡AÄIRIYOR: [Ã§aÄŸÄ±rdÄ±ÄŸÄ± fonksiyonlarÄ±n listesi]
Ã‡AÄRILIYOR_TARAFINDAN: [bu fonksiyonu Ã§aÄŸÄ±ran fonksiyonlar]

EÄŸer bir fonksiyon baÅŸka fonksiyon Ã§aÄŸÄ±rmÄ±yorsa "YOK" yaz.
"""
    }

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-3.5-turbo"):
        """
        OpenAI client'Ä± baÅŸlatÄ±r.

        Args:
            api_key: OpenAI API anahtarÄ±. None ise Ã§evre deÄŸiÅŸkeninden alÄ±nÄ±r.
            model: KullanÄ±lacak model (varsayÄ±lan: gpt-3.5-turbo)
        """
        # API anahtarÄ±nÄ± al (parametre > Ã§evre deÄŸiÅŸkeni > .env dosyasÄ±)
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model

        # OpenAI client'Ä± oluÅŸtur (kÃ¼tÃ¼phane mevcutsa)
        self.client = None
        if OPENAI_AVAILABLE and self.api_key:
            self.client = OpenAI(api_key=self.api_key)

    def is_available(self) -> bool:
        """
        OpenAI API'nin kullanÄ±labilir olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.

        Returns:
            True eÄŸer API anahtarÄ± ve kÃ¼tÃ¼phane mevcutsa
        """
        return OPENAI_AVAILABLE and self.api_key is not None and self.client is not None

    def generate_reasoning(self, code: str, prompt_type: str = "analysis") -> LLMResponse:
        """
        OpenAI API'sini kullanarak kod analizi yapar.

        Args:
            code: Analiz edilecek Python kodu
            prompt_type: Prompt tipi

        Returns:
            LLMResponse nesnesi

        Raises:
            RuntimeError: API kullanÄ±lamÄ±yorsa
        """
        if not self.is_available():
            raise RuntimeError("OpenAI API kullanÄ±lamÄ±yor. API anahtarÄ±nÄ± kontrol edin.")

        # Prompt ÅŸablonunu seÃ§ ve kodu yerleÅŸtir
        template = self.PROMPT_TEMPLATES.get(prompt_type, self.PROMPT_TEMPLATES["analysis"])
        prompt = template.format(code=code)

        # API Ã§aÄŸrÄ±sÄ± yap
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": "Sen deneyimli bir yazÄ±lÄ±m mÃ¼hendisisin. Kod analizi yaparken "
                               "her adÄ±mÄ± detaylÄ± aÃ§Ä±klarsÄ±n ve fonksiyonlar arasÄ± iliÅŸkileri "
                               "net bir ÅŸekilde belirtirsin."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,  # DÃ¼ÅŸÃ¼k sÄ±caklÄ±k = daha tutarlÄ± Ã§Ä±ktÄ±
            max_tokens=2000
        )

        # YanÄ±tÄ± parse et
        content = response.choices[0].message.content
        reasoning_steps = self._extract_reasoning_steps(content)

        return LLMResponse(
            content=content,
            reasoning_steps=reasoning_steps,
            model=self.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            },
            raw_response=response.model_dump()
        )

    def _extract_reasoning_steps(self, content: str) -> List[str]:
        """
        LLM yanÄ±tÄ±ndan muhakeme adÄ±mlarÄ±nÄ± Ã§Ä±karÄ±r.

        "ADIM X:" veya numaralÄ± maddeleri arar.

        Args:
            content: LLM yanÄ±t metni

        Returns:
            Muhakeme adÄ±mlarÄ± listesi
        """
        steps = []
        lines = content.split('\n')

        current_step = []
        for line in lines:
            line = line.strip()

            # Yeni adÄ±m baÅŸlangÄ±cÄ± mÄ±?
            if (line.startswith("ADIM") or
                line.startswith("1.") or line.startswith("2.") or
                line.startswith("3.") or line.startswith("4.") or
                line.startswith("FONKSIYON:") or
                line.startswith("GENEL BAKIÅ") or
                line.startswith("YAPISAL ANALÄ°Z")):

                # Ã–nceki adÄ±mÄ± kaydet
                if current_step:
                    steps.append('\n'.join(current_step))
                current_step = [line]
            elif current_step:
                current_step.append(line)

        # Son adÄ±mÄ± ekle
        if current_step:
            steps.append('\n'.join(current_step))

        return steps


class GeminiClient(BaseLLMClient):
    """
    Google Gemini API ile iletiÅŸim kuran client sÄ±nÄ±fÄ±.

    KullanÄ±m:
        client = GeminiClient(api_key="AIza...")
        response = client.generate_reasoning(kod, "analysis")
    """

    # Prompt ÅŸablonlarÄ± - JSON formatÄ±nda strict analiz
    PROMPT_TEMPLATES = {
        "analysis": """
Sen kÄ±demli bir statik kod analiz motorusun. GÃ¶revin, Python kodunun "Call Graph" (Ã‡aÄŸrÄ± GrafiÄŸi) yapÄ±sÄ±nÄ± Ã§Ä±karmaktÄ±r.

HEDEF:
Sadece sÃ¶zdizimsel (syntactic) olarak bir fonksiyonun GÃ–VDESÄ° Ä°Ã‡Ä°NDE Ã§aÄŸrÄ±lan diÄŸer fonksiyonlarÄ± tespit et.

KURALLAR (Ã‡ok Ã–nemli):
1. SIRALI Ã‡ALIÅMA != Ã‡AÄRI: EÄŸer A fonksiyonu Ã§alÄ±ÅŸÄ±p bittikten sonra B fonksiyonu Ã§alÄ±ÅŸÄ±yorsa, bu A'nÄ±n B'yi Ã§aÄŸÄ±rdÄ±ÄŸÄ± anlamÄ±na GELMEZ. Sadece A'nÄ±n gÃ¶vdesi iÃ§inde B() yazÄ±yorsa Ã§aÄŸÄ±rÄ±yor demektir.
2. RECURSION YOKSA KENDÄ°NÄ° EKLEME: Fonksiyon iÃ§inde kendi ismi aÃ§Ä±kÃ§a geÃ§miyorsa, kendini Ã§aÄŸÄ±rÄ±yor olarak iÅŸaretleme.
3. BUILT-IN DAHÄ°L ETME: print, len, range, open, str, int, float gibi gÃ¶mÃ¼lÃ¼ fonksiyonlarÄ± DAHÄ°L ETME. Sadece kodda tanÄ±mlÄ± fonksiyonlarÄ±/metotlarÄ± dikkate al.
4. METOT Ã‡AÄRILARI: self.method() ÅŸeklindeki Ã§aÄŸrÄ±larÄ± "method" olarak yaz (self. olmadan).
5. SINIF Ä°Ã‡Ä° METOTLAR: SÄ±nÄ±f metotlarÄ±nÄ± da fonksiyon olarak listele (Ã¶rn: Calculator.add -> "add").
6. Ã‡IKTI FORMATI: Sadece saf bir JSON nesnesi dÃ¶ndÃ¼r. AÃ§Ä±klama, yorum veya markdown (```) EKLEME.

JSON ÅEMASI (Bu formatÄ± AYNEN kullan):
{{
  "functions": [
    {{
      "name": "fonksiyon_adi",
      "calls": ["cagirdigi_fonksiyon_1", "cagirdigi_fonksiyon_2"]
    }}
  ]
}}

EÄŸer bir fonksiyon hiÃ§bir ÅŸey Ã§aÄŸÄ±rmÄ±yorsa: "calls": []

KOD:
```python
{code}
```

SADECE JSON DÃ–NDÃœR:
""",

        "explanation": """
AÅŸaÄŸÄ±daki Python kodunu bir yazÄ±lÄ±m geliÅŸtiriciye aÃ§Ä±klar gibi anlat.

KOD:
```python
{code}
```

AÃ§Ä±klamanÄ± ÅŸu baÅŸlÄ±klar altÄ±nda yap:
1. GENEL BAKIÅ: Kodun amacÄ± nedir?
2. YAPISAL ANALÄ°Z: Hangi fonksiyonlar/sÄ±nÄ±flar var?
3. Ã‡AÄRI Ä°LÄ°ÅKÄ°LERÄ°: Hangi fonksiyon hangisini Ã§aÄŸÄ±rÄ±yor?
4. VERÄ° AKIÅI: Veriler nasÄ±l iÅŸleniyor?
""",

        "function_calls": """
AÅŸaÄŸÄ±daki Python kodundaki fonksiyon Ã§aÄŸrÄ± iliÅŸkilerini analiz et.

KOD:
```python
{code}
```

Her fonksiyon iÃ§in ÅŸu formatta yanÄ±t ver:
FONKSIYON: [fonksiyon_adÄ±]
Ã‡AÄIRIYOR: [Ã§aÄŸÄ±rdÄ±ÄŸÄ± fonksiyonlarÄ±n listesi]

EÄŸer bir fonksiyon baÅŸka fonksiyon Ã§aÄŸÄ±rmÄ±yorsa "YOK" yaz.
Sadece kodda tanÄ±mlÄ± fonksiyonlarÄ± listele, print, len gibi built-in fonksiyonlarÄ± dahil etme.
"""
    }

    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-2.0-flash"):
        """
        Gemini client'Ä± baÅŸlatÄ±r.

        Args:
            api_key: Google AI API anahtarÄ±
            model: KullanÄ±lacak model (varsayÄ±lan: gemini-2.0-flash)
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        self.model_name = model
        self.model = None

        if GEMINI_AVAILABLE and self.api_key:
            # Gemini API'yi yapÄ±landÄ±r
            genai.configure(api_key=self.api_key)
            try:
                self.model = genai.GenerativeModel(self.model_name)
            except Exception as e:
                # Model bulunamazsa alternatif dene
                print(f"âš ï¸ {self.model_name} bulunamadÄ±, gemini-pro deneniyor...")
                self.model_name = "gemini-pro"
                self.model = genai.GenerativeModel(self.model_name)

    def is_available(self) -> bool:
        """Gemini API'nin kullanÄ±labilir olup olmadÄ±ÄŸÄ±nÄ± kontrol eder."""
        return GEMINI_AVAILABLE and self.api_key is not None and self.model is not None

    def generate_reasoning(self, code: str, prompt_type: str = "analysis") -> LLMResponse:
        """
        Gemini API'sini kullanarak kod analizi yapar.

        Args:
            code: Analiz edilecek Python kodu
            prompt_type: Prompt tipi

        Returns:
            LLMResponse nesnesi
        """
        if not self.is_available():
            raise RuntimeError("Gemini API kullanÄ±lamÄ±yor. API anahtarÄ±nÄ± kontrol edin.")

        # Prompt ÅŸablonunu seÃ§ ve kodu yerleÅŸtir
        template = self.PROMPT_TEMPLATES.get(prompt_type, self.PROMPT_TEMPLATES["analysis"])
        prompt = template.format(code=code)

        # API Ã§aÄŸrÄ±sÄ± yap (hata durumunda alternatif model dene veya retry)
        import time
        import re as regex_module

        alternative_models = ["gemini-1.5-flash-latest", "gemini-1.5-flash", "gemini-pro", "gemini-1.0-pro"]
        max_retries = 3

        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(prompt)
                break  # BaÅŸarÄ±lÄ±, dÃ¶ngÃ¼den Ã§Ä±k
            except Exception as e:
                error_msg = str(e)

                # 429 Rate Limit hatasÄ±
                if "429" in error_msg or "quota" in error_msg.lower() or "rate" in error_msg.lower():
                    # Bekleme sÃ¼resini Ã§Ä±kar
                    wait_match = regex_module.search(r'retry.*?(\d+\.?\d*)\s*s', error_msg.lower())
                    wait_time = float(wait_match.group(1)) if wait_match else 10

                    print(f"âš ï¸ Rate limit aÅŸÄ±ldÄ±. {wait_time:.1f} saniye bekleniyor... (Deneme {attempt + 1}/{max_retries})")
                    time.sleep(wait_time + 1)  # +1 gÃ¼venlik payÄ±

                    # FarklÄ± model dene (quota model bazlÄ± olabilir)
                    if attempt > 0 and attempt - 1 < len(alternative_models):
                        alt_model = alternative_models[attempt - 1]
                        print(f"   Alternatif model deneniyor: {alt_model}")
                        try:
                            self.model = genai.GenerativeModel(alt_model)
                            self.model_name = alt_model
                        except:
                            pass
                    continue

                # 404 Model bulunamadÄ± hatasÄ±
                elif "404" in error_msg or "not found" in error_msg.lower():
                    print(f"âš ï¸ {self.model_name} modeli bulunamadÄ±. Alternatif deneniyor...")
                    for alt_model in alternative_models:
                        try:
                            print(f"   Deneniyor: {alt_model}")
                            self.model = genai.GenerativeModel(alt_model)
                            self.model_name = alt_model
                            response = self.model.generate_content(prompt)
                            print(f"âœ… {alt_model} modeli Ã§alÄ±ÅŸtÄ±!")
                            break
                        except Exception:
                            continue
                    else:
                        raise RuntimeError(f"HiÃ§bir Gemini modeli Ã§alÄ±ÅŸmadÄ±. Hata: {error_msg}")
                    break
                else:
                    raise
        else:
            raise RuntimeError(f"Maksimum deneme sayÄ±sÄ±na ulaÅŸÄ±ldÄ±. LÃ¼tfen daha sonra tekrar deneyin veya API planÄ±nÄ±zÄ± kontrol edin.")

        # YanÄ±tÄ± al
        content = response.text
        reasoning_steps = self._extract_reasoning_steps(content)

        # Token bilgisi (Gemini'de farklÄ± ÅŸekilde alÄ±nÄ±yor)
        usage = {
            "prompt_tokens": len(prompt.split()),  # YaklaÅŸÄ±k
            "completion_tokens": len(content.split()),  # YaklaÅŸÄ±k
            "total_tokens": len(prompt.split()) + len(content.split())
        }

        return LLMResponse(
            content=content,
            reasoning_steps=reasoning_steps,
            model=self.model_name,
            usage=usage
        )

    def _extract_reasoning_steps(self, content: str) -> List[str]:
        """LLM yanÄ±tÄ±ndan muhakeme adÄ±mlarÄ±nÄ± Ã§Ä±karÄ±r."""
        steps = []
        lines = content.split('\n')

        current_step = []
        for line in lines:
            line = line.strip()

            # Yeni adÄ±m baÅŸlangÄ±cÄ± mÄ±?
            if (line.startswith("ADIM") or
                line.startswith("1.") or line.startswith("2.") or
                line.startswith("3.") or line.startswith("4.") or
                line.startswith("FONKSIYON:") or
                line.startswith("GENEL BAKIÅ") or
                line.startswith("YAPISAL ANALÄ°Z")):

                if current_step:
                    steps.append('\n'.join(current_step))
                current_step = [line]
            elif current_step:
                current_step.append(line)

        if current_step:
            steps.append('\n'.join(current_step))

        return steps


class GroqClient(BaseLLMClient):
    """
    Groq API ile iletiÅŸim kuran client sÄ±nÄ±fÄ±.

    Groq, LPU (Language Processing Unit) teknolojisi ile Ã§ok hÄ±zlÄ± inference saÄŸlar.
    Ãœcretsiz tier: 30 req/dakika, 14400 req/gÃ¼n

    KullanÄ±m:
        client = GroqClient(api_key="gsk_...")
        response = client.generate_reasoning(kod, "analysis")
    """

    # Prompt ÅŸablonlarÄ± - JSON formatÄ±nda strict analiz
    PROMPT_TEMPLATES = {
        "analysis": """
Sen kÄ±demli bir statik kod analiz motorusun. GÃ¶revin, Python kodunun "Call Graph" (Ã‡aÄŸrÄ± GrafiÄŸi) yapÄ±sÄ±nÄ± Ã§Ä±karmaktÄ±r.

HEDEF:
Sadece sÃ¶zdizimsel (syntactic) olarak bir fonksiyonun GÃ–VDESÄ° Ä°Ã‡Ä°NDE Ã§aÄŸrÄ±lan diÄŸer fonksiyonlarÄ± tespit et.

KURALLAR (Ã‡ok Ã–nemli):
1. SIRALI Ã‡ALIÅMA != Ã‡AÄRI: EÄŸer A fonksiyonu Ã§alÄ±ÅŸÄ±p bittikten sonra B fonksiyonu Ã§alÄ±ÅŸÄ±yorsa, bu A'nÄ±n B'yi Ã§aÄŸÄ±rdÄ±ÄŸÄ± anlamÄ±na GELMEZ. Sadece A'nÄ±n gÃ¶vdesi iÃ§inde B() yazÄ±yorsa Ã§aÄŸÄ±rÄ±yor demektir.
2. RECURSION YOKSA KENDÄ°NÄ° EKLEME: Fonksiyon iÃ§inde kendi ismi aÃ§Ä±kÃ§a geÃ§miyorsa, kendini Ã§aÄŸÄ±rÄ±yor olarak iÅŸaretleme.
3. BUILT-IN DAHÄ°L ETME: print, len, range, open, str, int, float gibi gÃ¶mÃ¼lÃ¼ fonksiyonlarÄ± DAHÄ°L ETME. Sadece kodda tanÄ±mlÄ± fonksiyonlarÄ±/metotlarÄ± dikkate al.
4. METOT Ã‡AÄRILARI: self.method() ÅŸeklindeki Ã§aÄŸrÄ±larÄ± "method" olarak yaz (self. olmadan).
5. SINIF Ä°Ã‡Ä° METOTLAR: SÄ±nÄ±f metotlarÄ±nÄ± da fonksiyon olarak listele (Ã¶rn: Calculator.add -> "add").
6. Ã‡IKTI FORMATI: Sadece saf bir JSON nesnesi dÃ¶ndÃ¼r. AÃ§Ä±klama, yorum veya markdown (```) EKLEME.

JSON ÅEMASI (Bu formatÄ± AYNEN kullan):
{{
  "functions": [
    {{
      "name": "fonksiyon_adi",
      "calls": ["cagirdigi_fonksiyon_1", "cagirdigi_fonksiyon_2"]
    }}
  ]
}}

EÄŸer bir fonksiyon hiÃ§bir ÅŸey Ã§aÄŸÄ±rmÄ±yorsa: "calls": []

KOD:
```python
{code}
```

SADECE JSON DÃ–NDÃœR:
""",
        "explanation": """
AÅŸaÄŸÄ±daki Python kodunu bir yazÄ±lÄ±m geliÅŸtiriciye aÃ§Ä±klar gibi anlat.

KOD:
```python
{code}
```

AÃ§Ä±klamanÄ± ÅŸu baÅŸlÄ±klar altÄ±nda yap:
1. GENEL BAKIÅ: Kodun amacÄ± nedir?
2. YAPISAL ANALÄ°Z: Hangi fonksiyonlar/sÄ±nÄ±flar var?
3. Ã‡AÄRI Ä°LÄ°ÅKÄ°LERÄ°: Hangi fonksiyon hangisini Ã§aÄŸÄ±rÄ±yor?
4. VERÄ° AKIÅI: Veriler nasÄ±l iÅŸleniyor?
""",
    }

    def __init__(self, api_key: Optional[str] = None, model: str = "llama-3.3-70b-versatile"):
        """
        Groq client'Ä± baÅŸlatÄ±r.

        Args:
            api_key: Groq API anahtarÄ±
            model: KullanÄ±lacak model (varsayÄ±lan: llama-3.3-70b-versatile)
        """
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        self.model = model
        self.client = None

        if GROQ_AVAILABLE and self.api_key:
            self.client = Groq(api_key=self.api_key)

    def is_available(self) -> bool:
        """Groq API'nin kullanÄ±labilir olup olmadÄ±ÄŸÄ±nÄ± kontrol eder."""
        return GROQ_AVAILABLE and self.api_key is not None and self.client is not None

    def generate_reasoning(self, code: str, prompt_type: str = "analysis") -> LLMResponse:
        """
        Groq API'sini kullanarak kod analizi yapar.

        Args:
            code: Analiz edilecek Python kodu
            prompt_type: Prompt tipi

        Returns:
            LLMResponse nesnesi
        """
        if not self.is_available():
            raise RuntimeError("Groq API kullanÄ±lamÄ±yor. API anahtarÄ±nÄ± kontrol edin.")

        # Prompt ÅŸablonunu seÃ§ ve kodu yerleÅŸtir
        template = self.PROMPT_TEMPLATES.get(prompt_type, self.PROMPT_TEMPLATES["analysis"])
        prompt = template.format(code=code)

        # API Ã§aÄŸrÄ±sÄ± yap
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": "Sen deneyimli bir yazÄ±lÄ±m mÃ¼hendisisin. Kod analizi yaparken "
                               "sadece JSON formatÄ±nda yanÄ±t verirsin. Markdown veya aÃ§Ä±klama ekleme."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.1,  # DÃ¼ÅŸÃ¼k sÄ±caklÄ±k = daha tutarlÄ± JSON Ã§Ä±ktÄ±sÄ±
            max_tokens=2000
        )

        # YanÄ±tÄ± parse et
        content = response.choices[0].message.content
        reasoning_steps = self._extract_reasoning_steps(content)

        return LLMResponse(
            content=content,
            reasoning_steps=reasoning_steps,
            model=self.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        )

    def _extract_reasoning_steps(self, content: str) -> List[str]:
        """LLM yanÄ±tÄ±ndan muhakeme adÄ±mlarÄ±nÄ± Ã§Ä±karÄ±r."""
        # JSON formatÄ±nda geldiÄŸi iÃ§in tek adÄ±m olarak dÃ¶ndÃ¼r
        return [content]


class MockLLMClient(BaseLLMClient):
    """
    Test amaÃ§lÄ± sahte (mock) LLM client.

    API anahtarÄ± olmadan test yapabilmek iÃ§in Ã¶nceden tanÄ±mlanmÄ±ÅŸ
    yanÄ±tlar dÃ¶ndÃ¼rÃ¼r. GeliÅŸtirme ve test sÃ¼recinde kullanÄ±ÅŸlÄ±dÄ±r.
    """

    def __init__(self):
        """Mock client'Ä± baÅŸlatÄ±r."""
        self.call_count = 0

    def is_available(self) -> bool:
        """Mock client her zaman kullanÄ±labilir."""
        return True

    def generate_reasoning(self, code: str, prompt_type: str = "analysis") -> LLMResponse:
        """
        Sahte bir LLM yanÄ±tÄ± Ã¼retir.

        Kod iÃ§eriÄŸini analiz ederek gerÃ§ekÃ§i gÃ¶rÃ¼nen bir yanÄ±t oluÅŸturur.
        Bu, API maliyeti olmadan sistemin test edilmesini saÄŸlar.

        Args:
            code: Analiz edilecek kod
            prompt_type: Prompt tipi

        Returns:
            Sahte LLMResponse
        """
        self.call_count += 1

        # Koddan basit bilgiler Ã§Ä±kar
        import re

        # Fonksiyon adlarÄ±nÄ± bul
        func_pattern = r'def\s+(\w+)\s*\('
        functions = re.findall(func_pattern, code)

        # SÄ±nÄ±f adlarÄ±nÄ± bul
        class_pattern = r'class\s+(\w+)'
        classes = re.findall(class_pattern, code)

        # Sahte analiz oluÅŸtur
        reasoning_content = self._generate_mock_analysis(functions, classes, code)

        return LLMResponse(
            content=reasoning_content,
            reasoning_steps=self._extract_steps_from_mock(reasoning_content),
            model="mock-model",
            usage={
                "prompt_tokens": len(code.split()),
                "completion_tokens": len(reasoning_content.split()),
                "total_tokens": len(code.split()) + len(reasoning_content.split())
            }
        )

    def _generate_mock_analysis(self, functions: List[str], classes: List[str], code: str) -> str:
        """
        Sahte analiz metni oluÅŸturur.

        Args:
            functions: Bulunan fonksiyon adlarÄ±
            classes: Bulunan sÄ±nÄ±f adlarÄ±
            code: Kaynak kod

        Returns:
            Analiz metni
        """
        import re

        analysis = []
        analysis.append("ADIM 1: GENEL BAKIÅ")
        analysis.append(f"Bu kod {len(functions)} fonksiyon ve {len(classes)} sÄ±nÄ±f iÃ§ermektedir.")
        analysis.append("")

        # Her fonksiyon iÃ§in analiz
        step = 2
        for func in functions:
            analysis.append(f"ADIM {step}: {func} FONKSÄ°YONU ANALÄ°ZÄ°")

            # Bu fonksiyonun Ã§aÄŸÄ±rdÄ±ÄŸÄ± diÄŸer fonksiyonlarÄ± bul
            # Basit regex ile fonksiyon Ã§aÄŸrÄ±larÄ±nÄ± ara
            func_body_match = re.search(
                rf'def\s+{func}\s*\([^)]*\):[^\n]*\n((?:\s+[^\n]+\n)*)',
                code
            )

            if func_body_match:
                func_body = func_body_match.group(1)
                # Fonksiyon Ã§aÄŸrÄ±larÄ±nÄ± bul
                calls = re.findall(r'(\w+)\s*\(', func_body)
                # Kendisi ve built-in'leri filtrele
                calls = [c for c in calls if c != func and c not in ('print', 'len', 'range', 'str', 'int', 'if', 'for', 'while')]
                calls = list(set(calls))

                if calls:
                    analysis.append(f"- {func} fonksiyonu ÅŸu fonksiyonlarÄ± Ã§aÄŸÄ±rÄ±yor: {', '.join(calls)}")
                else:
                    analysis.append(f"- {func} fonksiyonu baÅŸka fonksiyon Ã§aÄŸÄ±rmÄ±yor")
            else:
                analysis.append(f"- {func} fonksiyonu analiz edildi")

            analysis.append("")
            step += 1

        # SÄ±nÄ±flar iÃ§in analiz
        for cls in classes:
            analysis.append(f"ADIM {step}: {cls} SINIFI ANALÄ°ZÄ°")
            analysis.append(f"- {cls} sÄ±nÄ±fÄ± tanÄ±mlanmÄ±ÅŸ")

            # SÄ±nÄ±f metodlarÄ±nÄ± bul
            class_body_match = re.search(
                rf'class\s+{cls}[^:]*:((?:\n(?:\s+[^\n]+))*)',
                code
            )
            if class_body_match:
                class_body = class_body_match.group(1)
                methods = re.findall(r'def\s+(\w+)\s*\(self', class_body)
                if methods:
                    analysis.append(f"- MetodlarÄ±: {', '.join(methods)}")

            analysis.append("")
            step += 1

        # Ã‡aÄŸrÄ± iliÅŸkileri Ã¶zeti
        analysis.append(f"ADIM {step}: Ã‡AÄRI Ä°LÄ°ÅKÄ°LERÄ° Ã–ZETÄ°")
        for func in functions:
            analysis.append(f"FONKSIYON: {func}")
            # Basit Ã§aÄŸrÄ± analizi
            func_body_match = re.search(
                rf'def\s+{func}\s*\([^)]*\):[^\n]*\n((?:\s+[^\n]+\n)*)',
                code
            )
            if func_body_match:
                func_body = func_body_match.group(1)
                calls = re.findall(r'(\w+)\s*\(', func_body)
                valid_calls = [c for c in calls if c in functions and c != func]
                if valid_calls:
                    analysis.append(f"Ã‡AÄIRIYOR: {', '.join(valid_calls)}")
                else:
                    analysis.append("Ã‡AÄIRIYOR: YOK")
            analysis.append("")

        return '\n'.join(analysis)

    def _extract_steps_from_mock(self, content: str) -> List[str]:
        """Mock iÃ§erikten adÄ±mlarÄ± Ã§Ä±karÄ±r."""
        steps = []
        current_step = []

        for line in content.split('\n'):
            if line.startswith("ADIM") or line.startswith("FONKSIYON:"):
                if current_step:
                    steps.append('\n'.join(current_step))
                current_step = [line]
            elif current_step:
                current_step.append(line)

        if current_step:
            steps.append('\n'.join(current_step))

        return steps


class LLMClient:
    """
    LLM client'larÄ± iÃ§in fabrika sÄ±nÄ±fÄ±.

    Uygun client'Ä± otomatik olarak seÃ§er:
    - provider="groq": Groq API (Ã–NERÄ°LEN - Ã¼cretsiz ve hÄ±zlÄ±)
    - provider="gemini": Google Gemini API
    - provider="openai": OpenAI API
    - provider="mock": Test iÃ§in sahte client
    - provider="auto": Otomatik seÃ§im (Groq > Gemini > OpenAI > Mock)

    KullanÄ±m:
        client = LLMClient.create()  # Otomatik seÃ§im
        client = LLMClient.create(provider="groq", api_key="gsk_...")
        client = LLMClient.create(provider="gemini", api_key="AIza...")
        client = LLMClient.create(provider="openai", api_key="sk-...")
        client = LLMClient.create(provider="mock")  # Test iÃ§in
    """

    @staticmethod
    def create(provider: str = "auto", **kwargs) -> BaseLLMClient:
        """
        Uygun LLM client'Ä± oluÅŸturur.

        Args:
            provider: "groq", "gemini", "openai", "mock" veya "auto"
            **kwargs: Client'a geÃ§irilecek ek parametreler
                - api_key: API anahtarÄ±
                - model: Model adÄ±

        Returns:
            BaseLLMClient tÃ¼revi client
        """
        # Mock client istendi
        if provider == "mock":
            return MockLLMClient()

        # Groq client (Ã–NERÄ°LEN)
        if provider == "groq":
            api_key = kwargs.get("api_key") or os.getenv("GROQ_API_KEY")
            if api_key and GROQ_AVAILABLE:
                print("âœ… Groq API kullanÄ±lÄ±yor (Llama 3.3 70B).")
                return GroqClient(api_key=api_key, model=kwargs.get("model", "llama-3.3-70b-versatile"))
            else:
                raise RuntimeError("Groq API anahtarÄ± bulunamadÄ± veya kÃ¼tÃ¼phane kurulu deÄŸil. "
                                   "Kurulum: pip install groq")

        # Gemini client
        if provider == "gemini":
            api_key = kwargs.get("api_key") or os.getenv("GEMINI_API_KEY")
            if api_key and GEMINI_AVAILABLE:
                print("âœ… Google Gemini API kullanÄ±lÄ±yor.")
                return GeminiClient(api_key=api_key, model=kwargs.get("model", "gemini-2.0-flash"))
            else:
                raise RuntimeError("Gemini API anahtarÄ± bulunamadÄ± veya kÃ¼tÃ¼phane kurulu deÄŸil.")

        # OpenAI client
        if provider == "openai":
            api_key = kwargs.get("api_key") or os.getenv("OPENAI_API_KEY")
            if api_key and OPENAI_AVAILABLE:
                print("âœ… OpenAI API kullanÄ±lÄ±yor.")
                return OpenAIClient(api_key=api_key, model=kwargs.get("model", "gpt-3.5-turbo"))
            else:
                raise RuntimeError("OpenAI API anahtarÄ± bulunamadÄ± veya kÃ¼tÃ¼phane kurulu deÄŸil.")

        # Auto mod: SÄ±rayla dene (Groq > Gemini > OpenAI > Mock)
        if provider == "auto":
            api_key = kwargs.get("api_key")

            # Ã–nce Groq dene (Ã¼cretsiz ve hÄ±zlÄ±)
            groq_key = api_key or os.getenv("GROQ_API_KEY")
            if groq_key and GROQ_AVAILABLE:
                print("âœ… Groq API kullanÄ±lÄ±yor (auto) - Llama 3.3 70B.")
                return GroqClient(api_key=groq_key, model=kwargs.get("model", "llama-3.3-70b-versatile"))

            # Sonra Gemini dene
            gemini_key = api_key or os.getenv("GEMINI_API_KEY")
            if gemini_key and GEMINI_AVAILABLE:
                print("âœ… Google Gemini API kullanÄ±lÄ±yor (auto).")
                return GeminiClient(api_key=gemini_key, model=kwargs.get("model", "gemini-2.0-flash"))

            # Sonra OpenAI dene
            openai_key = api_key or os.getenv("OPENAI_API_KEY")
            if openai_key and OPENAI_AVAILABLE:
                print("âœ… OpenAI API kullanÄ±lÄ±yor (auto).")
                return OpenAIClient(api_key=openai_key, model=kwargs.get("model", "gpt-3.5-turbo"))

        # HiÃ§biri yoksa Mock kullan
        print("â„¹ï¸  API bulunamadÄ±, Mock mod kullanÄ±lÄ±yor.")
        return MockLLMClient()


# =============================================================================
# TEST KODU
# =============================================================================
if __name__ == "__main__":
    # Test kodu
    test_code = '''
class Calculator:
    def __init__(self):
        self.result = 0

    def add(self, a, b):
        self.result = a + b
        self._log("add")
        return self.result

    def _log(self, operation):
        print(f"Ä°ÅŸlem: {operation}, SonuÃ§: {self.result}")

def process_numbers(numbers):
    calc = Calculator()
    total = 0
    for num in numbers:
        total = calc.add(total, num)
    return total

def main():
    data = [1, 2, 3, 4, 5]
    result = process_numbers(data)
    print(f"Toplam: {result}")
    save_to_file(result)

def save_to_file(value):
    with open("output.txt", "w") as f:
        f.write(str(value))
'''

    print("=" * 60)
    print("LLM CLIENT TESTÄ°")
    print("=" * 60)

    # Client oluÅŸtur (otomatik seÃ§im)
    client = LLMClient.create()
    print(f"\nğŸ“¡ KullanÄ±lan client: {type(client).__name__}")
    print(f"   KullanÄ±labilir: {client.is_available()}")

    # Analiz yap
    print("\nğŸ” Kod analizi yapÄ±lÄ±yor...")
    response = client.generate_reasoning(test_code, "analysis")

    print(f"\nğŸ“Š Token kullanÄ±mÄ±: {response.usage}")
    print(f"ğŸ¤– Model: {response.model}")

    print("\n" + "=" * 60)
    print("LLM YANITI")
    print("=" * 60)
    print(response.content)

    print("\n" + "=" * 60)
    print("Ã‡IKARILAN ADIMLAR")
    print("=" * 60)
    for i, step in enumerate(response.reasoning_steps, 1):
        print(f"\n--- AdÄ±m {i} ---")
        print(step[:200] + "..." if len(step) > 200 else step)
