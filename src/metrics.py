# =============================================================================
# METRICS (METRÄ°KLER) MODÃœLÃœ
# =============================================================================
# Bu modÃ¼l, doÄŸrulama sÃ¼recinin performansÄ±nÄ± Ã¶lÃ§en metrikleri hesaplar.
#
# Hesaplanan Metrikler:
# --------------------
# 1. HALLUCINATION RATE (HalÃ¼sinasyon OranÄ±):
#    YanlÄ±ÅŸ/desteklenmeyen iddialarÄ±n toplam iddialara oranÄ±
#    FormÃ¼l: hallucination_count / total_claims
#
# 2. VALIDITY RATE (GeÃ§erlilik OranÄ±):
#    DoÄŸru iddialarÄ±n toplam iddialara oranÄ±
#    FormÃ¼l: (valid + partially_valid) / total_claims
#
# 3. COVERAGE (Kapsam):
#    LLM'nin bahsettiÄŸi kod yapÄ±larÄ±nÄ±n gerÃ§ek yapÄ±lara oranÄ±
#    FormÃ¼l: mentioned_entities / total_code_entities
#
# 4. STEP VALIDITY (AdÄ±m GeÃ§erliliÄŸi):
#    Her reasoning adÄ±mÄ±nÄ±n doÄŸruluk durumu
#
# 5. CHAIN COHERENCE (Zincir TutarlÄ±lÄ±ÄŸÄ±):
#    ArdÄ±ÅŸÄ±k adÄ±mlar arasÄ±ndaki tutarlÄ±lÄ±k
#
# 6. PRECISION & RECALL:
#    - Precision: DoÄŸru pozitif / (DoÄŸru pozitif + YanlÄ±ÅŸ pozitif)
#    - Recall: DoÄŸru pozitif / (DoÄŸru pozitif + YanlÄ±ÅŸ negatif)
# =============================================================================

from typing import Dict, List, Any, Optional, Set
from dataclasses import dataclass, field
from collections import defaultdict

from .verifier import VerificationDetail, VerificationResult, VerificationReport
from .claim_extractor import Claim, ClaimType


@dataclass
class MetricResult:
    """
    Tek bir metrik sonucunu temsil eden sÄ±nÄ±f.

    Attributes:
        name: Metrik adÄ±
        value: Metrik deÄŸeri (0-1 arasÄ± oran veya sayÄ±)
        description: Metrik aÃ§Ä±klamasÄ±
        details: DetaylÄ± bilgiler
    """
    name: str
    value: float
    description: str
    details: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """SÃ¶zlÃ¼k formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r."""
        return {
            "name": self.name,
            "value": self.value,
            "description": self.description,
            "details": self.details
        }

    def as_percentage(self) -> str:
        """YÃ¼zde olarak formatlar."""
        return f"{self.value * 100:.1f}%"


@dataclass
class MetricsReport:
    """
    TÃ¼m metriklerin raporunu tutan sÄ±nÄ±f.

    Attributes:
        metrics: Hesaplanan metrikler listesi
        summary: Ã–zet bilgiler
        recommendations: Ä°yileÅŸtirme Ã¶nerileri
    """
    metrics: List[MetricResult] = field(default_factory=list)
    summary: Dict[str, Any] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """SÃ¶zlÃ¼k formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r."""
        return {
            "metrics": [m.to_dict() for m in self.metrics],
            "summary": self.summary,
            "recommendations": self.recommendations
        }

    def get_metric(self, name: str) -> Optional[MetricResult]:
        """Ä°sme gÃ¶re metrik dÃ¶ndÃ¼rÃ¼r."""
        for metric in self.metrics:
            if metric.name == name:
                return metric
        return None


class MetricsCalculator:
    """
    DoÄŸrulama metriklerini hesaplayan sÄ±nÄ±f.

    Bu sÄ±nÄ±f, VerificationReport'tan metrikleri hesaplar ve
    LLM'nin gÃ¼venilirliÄŸi hakkÄ±nda sayÄ±sal deÄŸerlendirmeler sunar.

    KullanÄ±m:
        calculator = MetricsCalculator()
        report = calculator.calculate(verification_report, code_entities)

        print(f"HalÃ¼sinasyon OranÄ±: {report.get_metric('hallucination_rate').as_percentage()}")
    """

    def __init__(self):
        """MetricsCalculator'Ä± baÅŸlatÄ±r."""
        self.metrics: List[MetricResult] = []
        self.verification_report: Optional[VerificationReport] = None
        self.code_entities: Set[str] = set()

    def calculate(self, verification_report: VerificationReport,
                  code_entities: Optional[Set[str]] = None) -> MetricsReport:
        """
        TÃ¼m metrikleri hesaplar.

        Args:
            verification_report: DoÄŸrulama raporu
            code_entities: Koddaki tÃ¼m varlÄ±klar (coverage iÃ§in)

        Returns:
            MetricsReport nesnesi
        """
        self.verification_report = verification_report
        self.code_entities = code_entities or set()
        self.metrics = []

        # Metrikleri hesapla
        self._calculate_hallucination_rate()
        self._calculate_validity_rate()
        self._calculate_coverage()
        self._calculate_step_validity()
        self._calculate_claim_type_breakdown()
        self._calculate_confidence_distribution()
        self._calculate_chain_coherence()

        # Ã–zet ve Ã¶neriler oluÅŸtur
        summary = self._create_summary()
        recommendations = self._generate_recommendations()

        return MetricsReport(
            metrics=self.metrics,
            summary=summary,
            recommendations=recommendations
        )

    def _calculate_hallucination_rate(self):
        """
        HalÃ¼sinasyon oranÄ±nÄ± hesaplar.

        HalÃ¼sinasyon oranÄ±, LLM'nin ne kadar sÄ±klÄ±kla yanlÄ±ÅŸ
        veya desteklenmeyen iddialar Ã¼rettiÄŸini gÃ¶sterir.

        DÃ¼ÅŸÃ¼k oran (< 0.1): Ä°yi, LLM gÃ¼venilir
        Orta oran (0.1-0.3): Dikkatli olunmalÄ±
        YÃ¼ksek oran (> 0.3): Sorunlu, LLM Ã§Ä±ktÄ±larÄ± gÃ¼venilmez
        """
        details = self.verification_report.summary
        total = details.get("total_claims", 0)
        hallucinations = details.get("hallucination_count", 0)

        rate = hallucinations / total if total > 0 else 0

        self.metrics.append(MetricResult(
            name="hallucination_rate",
            value=rate,
            description="HalÃ¼sinasyon (yanlÄ±ÅŸ iddia) oranÄ±. "
                       "DÃ¼ÅŸÃ¼k deÄŸer daha iyi (< 0.1 ideal).",
            details={
                "total_claims": total,
                "hallucination_count": hallucinations,
                "threshold_good": 0.1,
                "threshold_bad": 0.3
            }
        ))

    def _calculate_validity_rate(self):
        """
        GeÃ§erlilik oranÄ±nÄ± hesaplar.

        GeÃ§erlilik oranÄ±, LLM'nin doÄŸru iddialarÄ±n toplam
        iddialara oranÄ±dÄ±r.

        YÃ¼ksek oran (> 0.8): Ä°yi
        Orta oran (0.5-0.8): Kabul edilebilir
        DÃ¼ÅŸÃ¼k oran (< 0.5): Sorunlu
        """
        details = self.verification_report.summary
        total = details.get("total_claims", 0)
        valid = details.get("valid_count", 0)
        partially_valid = details.get("partially_valid_count", 0)

        rate = (valid + partially_valid) / total if total > 0 else 0

        self.metrics.append(MetricResult(
            name="validity_rate",
            value=rate,
            description="GeÃ§erli iddialarÄ±n oranÄ±. "
                       "YÃ¼ksek deÄŸer daha iyi (> 0.8 ideal).",
            details={
                "total_claims": total,
                "valid_count": valid,
                "partially_valid_count": partially_valid,
                "threshold_good": 0.8,
                "threshold_bad": 0.5
            }
        ))

    def _calculate_coverage(self):
        """
        Kapsam metriÄŸini hesaplar.

        Kapsam, LLM'nin bahsettiÄŸi kod varlÄ±klarÄ±nÄ±n gerÃ§ek
        kod varlÄ±klarÄ±na oranÄ±dÄ±r.

        YÃ¼ksek kapsam: LLM kodun bÃ¼yÃ¼k bÃ¶lÃ¼mÃ¼nÃ¼ analiz etti
        DÃ¼ÅŸÃ¼k kapsam: LLM kodun sadece bir kÄ±smÄ±ndan bahsetti
        """
        if not self.code_entities:
            self.metrics.append(MetricResult(
                name="coverage",
                value=0,
                description="Kod kapsamÄ± hesaplanamadÄ± (kod varlÄ±klarÄ± saÄŸlanmadÄ±).",
                details={"error": "no_code_entities"}
            ))
            return

        # LLM'nin bahsettiÄŸi varlÄ±klarÄ± topla
        mentioned_entities = set()
        for detail in self.verification_report.details:
            if detail.subject_match and detail.subject_match.code_entity:
                mentioned_entities.add(detail.subject_match.code_entity)
            if detail.object_match and detail.object_match.code_entity:
                mentioned_entities.add(detail.object_match.code_entity)

        total_entities = len(self.code_entities)
        mentioned_count = len(mentioned_entities & self.code_entities)

        coverage = mentioned_count / total_entities if total_entities > 0 else 0

        self.metrics.append(MetricResult(
            name="coverage",
            value=coverage,
            description="LLM'nin analiz ettiÄŸi kod yapÄ±larÄ±nÄ±n oranÄ±.",
            details={
                "total_code_entities": total_entities,
                "mentioned_entities": mentioned_count,
                "mentioned_list": list(mentioned_entities & self.code_entities)
            }
        ))

    def _calculate_step_validity(self):
        """
        Her reasoning adÄ±mÄ±nÄ±n geÃ§erlilik durumunu hesaplar.

        Bu metrik, hangi adÄ±mlarda hata yapÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir.
        """
        # AdÄ±mlara gÃ¶re grupla
        steps: Dict[int, List[VerificationDetail]] = defaultdict(list)
        for detail in self.verification_report.details:
            step = detail.claim.source_step
            steps[step].append(detail)

        # Her adÄ±m iÃ§in geÃ§erlilik hesapla
        step_validity = {}
        for step_num, details in sorted(steps.items()):
            valid_count = sum(1 for d in details if d.is_valid())
            total_count = len(details)
            validity = valid_count / total_count if total_count > 0 else 0
            step_validity[step_num] = {
                "validity_rate": validity,
                "valid_count": valid_count,
                "total_count": total_count
            }

        # Ortalama adÄ±m geÃ§erliliÄŸi
        avg_validity = sum(s["validity_rate"] for s in step_validity.values()) / len(step_validity) if step_validity else 0

        self.metrics.append(MetricResult(
            name="step_validity",
            value=avg_validity,
            description="Reasoning adÄ±mlarÄ±nÄ±n ortalama geÃ§erlilik oranÄ±.",
            details={
                "per_step": step_validity,
                "total_steps": len(step_validity)
            }
        ))

    def _calculate_claim_type_breakdown(self):
        """
        Claim tÃ¼rlerine gÃ¶re baÅŸarÄ± oranlarÄ±nÄ± hesaplar.

        Her claim tÃ¼rÃ¼ (CALL, DATA_FLOW, EXISTENCE vb.) iÃ§in
        ayrÄ± ayrÄ± baÅŸarÄ± oranlarÄ± gÃ¶sterilir.
        """
        # TÃ¼rlere gÃ¶re grupla
        by_type: Dict[str, Dict[str, int]] = defaultdict(lambda: {
            "total": 0, "valid": 0, "hallucination": 0, "other": 0
        })

        for detail in self.verification_report.details:
            claim_type = detail.claim.claim_type.value
            by_type[claim_type]["total"] += 1

            if detail.is_valid():
                by_type[claim_type]["valid"] += 1
            elif detail.is_hallucination():
                by_type[claim_type]["hallucination"] += 1
            else:
                by_type[claim_type]["other"] += 1

        # Her tÃ¼r iÃ§in oran hesapla
        type_breakdown = {}
        for claim_type, counts in by_type.items():
            total = counts["total"]
            type_breakdown[claim_type] = {
                "total": total,
                "validity_rate": counts["valid"] / total if total > 0 else 0,
                "hallucination_rate": counts["hallucination"] / total if total > 0 else 0
            }

        self.metrics.append(MetricResult(
            name="claim_type_breakdown",
            value=len(type_breakdown),  # TÃ¼r sayÄ±sÄ±
            description="Claim tÃ¼rlerine gÃ¶re baÅŸarÄ± oranlarÄ±.",
            details=type_breakdown
        ))

    def _calculate_confidence_distribution(self):
        """
        GÃ¼ven skoru daÄŸÄ±lÄ±mÄ±nÄ± hesaplar.

        DoÄŸrulama sonuÃ§larÄ±nÄ±n gÃ¼ven skorlarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶sterir.
        """
        confidences = [d.confidence for d in self.verification_report.details]

        if not confidences:
            self.metrics.append(MetricResult(
                name="confidence_distribution",
                value=0,
                description="GÃ¼ven skoru daÄŸÄ±lÄ±mÄ± hesaplanamadÄ±.",
                details={"error": "no_data"}
            ))
            return

        # Ä°statistikler
        avg_confidence = sum(confidences) / len(confidences)
        min_confidence = min(confidences)
        max_confidence = max(confidences)

        # DaÄŸÄ±lÄ±m gruplarÄ±
        distribution = {
            "low (0-0.3)": sum(1 for c in confidences if c < 0.3),
            "medium (0.3-0.7)": sum(1 for c in confidences if 0.3 <= c < 0.7),
            "high (0.7-1.0)": sum(1 for c in confidences if c >= 0.7)
        }

        self.metrics.append(MetricResult(
            name="confidence_distribution",
            value=avg_confidence,
            description="DoÄŸrulama sonuÃ§larÄ±nÄ±n ortalama gÃ¼ven skoru.",
            details={
                "average": avg_confidence,
                "min": min_confidence,
                "max": max_confidence,
                "distribution": distribution,
                "total_claims": len(confidences)
            }
        ))

    def _calculate_chain_coherence(self):
        """
        Zincir tutarlÄ±lÄ±ÄŸÄ±nÄ± hesaplar.

        ArdÄ±ÅŸÄ±k reasoning adÄ±mlarÄ± arasÄ±ndaki tutarlÄ±lÄ±ÄŸÄ± Ã¶lÃ§er.
        EÄŸer bir adÄ±mda halÃ¼sinasyon varsa ve sonraki adÄ±m ona
        dayalÄ±ysa, tutarlÄ±lÄ±k dÃ¼ÅŸer.
        """
        details = self.verification_report.details
        if len(details) < 2:
            self.metrics.append(MetricResult(
                name="chain_coherence",
                value=1.0,
                description="Zincir tutarlÄ±lÄ±ÄŸÄ± (yeterli veri yok).",
                details={"insufficient_data": True}
            ))
            return

        # AdÄ±mlara gÃ¶re sÄ±rala
        sorted_details = sorted(details, key=lambda d: d.claim.source_step)

        # ArdÄ±ÅŸÄ±k adÄ±mlar arasÄ±nda tutarlÄ±lÄ±k kontrolÃ¼
        coherent_transitions = 0
        total_transitions = 0

        for i in range(len(sorted_details) - 1):
            current = sorted_details[i]
            next_detail = sorted_details[i + 1]

            total_transitions += 1

            # Her ikisi de valid veya her ikisi de invalid ise tutarlÄ±
            if current.is_valid() == next_detail.is_valid():
                coherent_transitions += 1
            # Ã–nceki halÃ¼sinasyon, sonraki de halÃ¼sinasyon ise (hata yayÄ±lÄ±mÄ±)
            elif current.is_hallucination() and next_detail.is_hallucination():
                coherent_transitions += 0.5  # KÄ±smen tutarlÄ± (kÃ¶tÃ¼ ÅŸekilde)

        coherence = coherent_transitions / total_transitions if total_transitions > 0 else 1.0

        self.metrics.append(MetricResult(
            name="chain_coherence",
            value=coherence,
            description="ArdÄ±ÅŸÄ±k reasoning adÄ±mlarÄ± arasÄ±ndaki tutarlÄ±lÄ±k.",
            details={
                "total_transitions": total_transitions,
                "coherent_transitions": coherent_transitions
            }
        ))

    def _create_summary(self) -> Dict[str, Any]:
        """
        Ã–zet bilgileri oluÅŸturur.

        Returns:
            Ã–zet sÃ¶zlÃ¼ÄŸÃ¼
        """
        hallucination_rate = self._get_metric_value("hallucination_rate")
        validity_rate = self._get_metric_value("validity_rate")

        # Genel deÄŸerlendirme
        if hallucination_rate < 0.1 and validity_rate > 0.8:
            overall_assessment = "MÃœKEMMEL"
            assessment_color = "green"
        elif hallucination_rate < 0.2 and validity_rate > 0.6:
            overall_assessment = "Ä°YÄ°"
            assessment_color = "yellow"
        elif hallucination_rate < 0.3:
            overall_assessment = "ORTA"
            assessment_color = "orange"
        else:
            overall_assessment = "ZAYIF"
            assessment_color = "red"

        return {
            "overall_assessment": overall_assessment,
            "assessment_color": assessment_color,
            "hallucination_rate": hallucination_rate,
            "validity_rate": validity_rate,
            "total_claims": self.verification_report.summary.get("total_claims", 0),
            "hallucination_count": self.verification_report.summary.get("hallucination_count", 0)
        }

    def _generate_recommendations(self) -> List[str]:
        """
        Ä°yileÅŸtirme Ã¶nerileri oluÅŸturur.

        Metriklere gÃ¶re spesifik Ã¶neriler sunar.

        Returns:
            Ã–neri listesi
        """
        recommendations = []

        hallucination_rate = self._get_metric_value("hallucination_rate")
        validity_rate = self._get_metric_value("validity_rate")
        coverage = self._get_metric_value("coverage")

        # HalÃ¼sinasyon oranÄ±na gÃ¶re Ã¶neriler
        if hallucination_rate > 0.3:
            recommendations.append(
                "âš ï¸  YÃ¼ksek halÃ¼sinasyon oranÄ±! LLM Ã§Ä±ktÄ±larÄ±na gÃ¼venmeyin. "
                "Daha spesifik promptlar kullanmayÄ± deneyin."
            )
        elif hallucination_rate > 0.1:
            recommendations.append(
                "ğŸ“ Orta dÃ¼zeyde halÃ¼sinasyon oranÄ±. "
                "Kritik kararlar iÃ§in LLM Ã§Ä±ktÄ±larÄ±nÄ± manuel doÄŸrulayÄ±n."
            )

        # GeÃ§erlilik oranÄ±na gÃ¶re Ã¶neriler
        if validity_rate < 0.5:
            recommendations.append(
                "âŒ DÃ¼ÅŸÃ¼k geÃ§erlilik oranÄ±. LLM'nin kod anlayÄ±ÅŸÄ± yetersiz. "
                "Daha basit kod yapÄ±larÄ± veya daha gÃ¼Ã§lÃ¼ model deneyin."
            )

        # Kapsama gÃ¶re Ã¶neriler
        if coverage < 0.5:
            recommendations.append(
                "ğŸ“Š DÃ¼ÅŸÃ¼k kapsam. LLM kodun sadece bir kÄ±smÄ±nÄ± analiz etti. "
                "TÃ¼m yapÄ±larÄ± sorgulamak iÃ§in ek promptlar kullanÄ±n."
            )

        # Claim tÃ¼rÃ¼ baÅŸarÄ± oranlarÄ±na gÃ¶re Ã¶neriler
        type_metric = self._get_metric("claim_type_breakdown")
        if type_metric and type_metric.details:
            for claim_type, stats in type_metric.details.items():
                if isinstance(stats, dict) and stats.get("hallucination_rate", 0) > 0.4:
                    recommendations.append(
                        f"ğŸ” '{claim_type}' tÃ¼rÃ¼ndeki iddialar iÃ§in yÃ¼ksek hata oranÄ±. "
                        f"Bu tÃ¼r iddialarÄ± Ã¶zellikle doÄŸrulayÄ±n."
                    )

        # HiÃ§ Ã¶neri yoksa pozitif mesaj
        if not recommendations:
            recommendations.append(
                "âœ… LLM Ã§Ä±ktÄ±larÄ± genel olarak gÃ¼venilir gÃ¶rÃ¼nÃ¼yor. "
                "Yine de kritik kararlar iÃ§in manuel doÄŸrulama Ã¶nerilir."
            )

        return recommendations

    def _get_metric_value(self, name: str) -> float:
        """Ä°sme gÃ¶re metrik deÄŸeri dÃ¶ndÃ¼rÃ¼r."""
        for metric in self.metrics:
            if metric.name == name:
                return metric.value
        return 0.0

    def _get_metric(self, name: str) -> Optional[MetricResult]:
        """Ä°sme gÃ¶re metrik dÃ¶ndÃ¼rÃ¼r."""
        for metric in self.metrics:
            if metric.name == name:
                return metric
        return None

    def print_report(self, metrics_report: MetricsReport):
        """
        Metrik raporunu konsola yazdÄ±rÄ±r.

        Args:
            metrics_report: MetricsReport nesnesi
        """
        print("=" * 70)
        print("METRÄ°K RAPORU")
        print("=" * 70)

        # Genel deÄŸerlendirme
        summary = metrics_report.summary
        print(f"\nğŸ¯ GENEL DEÄERLENDÄ°RME: {summary['overall_assessment']}")

        # Ana metrikler
        print("\nğŸ“Š ANA METRÄ°KLER:")
        print("-" * 50)

        for metric in metrics_report.metrics:
            if metric.name in ["hallucination_rate", "validity_rate", "coverage",
                              "step_validity", "chain_coherence", "confidence_distribution"]:
                print(f"\n   {metric.name}:")
                print(f"      DeÄŸer: {metric.as_percentage()}")
                print(f"      AÃ§Ä±klama: {metric.description}")

        # Claim tÃ¼rÃ¼ analizi
        type_metric = metrics_report.get_metric("claim_type_breakdown")
        if type_metric and isinstance(type_metric.details, dict):
            print("\nğŸ“ˆ CLAIM TÃœRÃœ ANALÄ°ZÄ°:")
            print("-" * 50)
            for claim_type, stats in type_metric.details.items():
                if isinstance(stats, dict):
                    print(f"\n   {claim_type}:")
                    print(f"      Toplam: {stats.get('total', 0)}")
                    print(f"      GeÃ§erlilik: {stats.get('validity_rate', 0)*100:.1f}%")
                    print(f"      HalÃ¼sinasyon: {stats.get('hallucination_rate', 0)*100:.1f}%")

        # Ã–neriler
        print("\nğŸ’¡ Ã–NERÄ°LER:")
        print("-" * 50)
        for rec in metrics_report.recommendations:
            print(f"   {rec}")

        print("\n" + "=" * 70)


# =============================================================================
# TEST KODU
# =============================================================================
if __name__ == "__main__":
    from .verifier import VerificationReport, VerificationDetail, VerificationResult
    from .claim_extractor import Claim, ClaimType
    from .entity_mapper import EntityMatch, MatchType

    # Test verisi oluÅŸtur
    test_details = [
        VerificationDetail(
            claim=Claim("main calls process", ClaimType.CALL, "main", "process", "calls", source_step=1),
            result=VerificationResult.VALID,
            confidence=0.95,
            reason="DoÄŸrulandÄ±"
        ),
        VerificationDetail(
            claim=Claim("calc add", ClaimType.CALL, "calc", "add", "calls", source_step=1),
            result=VerificationResult.VALID,
            confidence=0.85,
            reason="DoÄŸrulandÄ±"
        ),
        VerificationDetail(
            claim=Claim("main save_result", ClaimType.CALL, "main", "save_result", "calls", source_step=2),
            result=VerificationResult.HALLUCINATION,
            confidence=0.9,
            reason="save_result yok"
        ),
        VerificationDetail(
            claim=Claim("DataProcessor exists", ClaimType.EXISTENCE, "DataProcessor", None, "exists", source_step=3),
            result=VerificationResult.HALLUCINATION,
            confidence=0.85,
            reason="SÄ±nÄ±f yok"
        ),
        VerificationDetail(
            claim=Claim("Calculator exists", ClaimType.EXISTENCE, "Calculator", None, "exists", source_step=3),
            result=VerificationResult.VALID,
            confidence=1.0,
            reason="Mevcut"
        ),
    ]

    test_report = VerificationReport(
        details=test_details,
        summary={
            "total_claims": 5,
            "valid_count": 3,
            "hallucination_count": 2,
            "unverifiable_count": 0,
            "partially_valid_count": 0
        },
        hallucinations=[d for d in test_details if d.is_hallucination()]
    )

    # Metrikleri hesapla
    calculator = MetricsCalculator()
    code_entities = {"main", "process_data", "Calculator", "add", "_validate"}
    metrics_report = calculator.calculate(test_report, code_entities)

    # Raporu yazdÄ±r
    calculator.print_report(metrics_report)
